import pandas as pd
import aiohttp
import asyncio
import tempfile
import os
from io import StringIO
import requests
import yaml
from tqdm.asyncio import tqdm_asyncio

# -------------------------
# è¯»å–é…ç½®
# -------------------------
def load_config():
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    if os.path.exists("config_local.yaml"):
        with open("config_local.yaml", "r", encoding="utf-8") as f:
            local_config = yaml.safe_load(f)
            config.update(local_config)
    return config

config = load_config()

CSV_URL = config.get("csv_url")
MAX_CONCURRENT = config.get("max_concurrent", 5)
DOWNLOAD_IF_MISS = config.get("download_if_miss", True)
HEAD_WAIT = config.get("head_wait_seconds", 1)
COLUMNS = config.get("columns", ["url", "cover", "lrc"])
SAVE_FILE = config.get("keep_downloaded_file", False)
DOWNLOAD_DIR = config.get("download_dir", "downloads")
AUTO_PURGE_CF_CACHE = config.get("auto_purge_cf_cache", False)
OUTPUT_CSV = config.get("output_csv", "output_cache_status.csv")

# Cloudflare API é…ç½®
CF_API_URL = config.get("cf_api_url")
CF_API_TOKEN = config.get("cf_api_token")
CF_ZONE_ID = config.get("cf_zone_id")

# -------------------------
# åˆ¤æ–­ CSV_URL æ˜¯æœ¬åœ°è¿˜æ˜¯åœ¨çº¿
# -------------------------
if CSV_URL.startswith("http"):
    r = requests.get(CSV_URL)
    r.raise_for_status()
    df = pd.read_csv(StringIO(r.text))
    print("åœ¨çº¿è¡¨æ ¼å·²åŠ è½½:", df.shape)
elif os.path.exists(CSV_URL):
    df = pd.read_csv(CSV_URL)
    print("æœ¬åœ° CSV å·²åŠ è½½:", df.shape)
else:
    raise ValueError(f"CSV_URL æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨: {CSV_URL}")

# ç¡®ä¿ COLUMNS ä¸­çš„åˆ—å­˜åœ¨
for col in COLUMNS:
    if col not in df.columns:
        raise ValueError(f"åˆ— '{col}' åœ¨ CSV ä¸­ä¸å­˜åœ¨")

# -------------------------
# æ‰¹é‡ CF æ¸…é™¤ç¼“å­˜
# -------------------------
async def purge_cf_cache(urls):
    if not all([CF_API_URL, CF_API_TOKEN, CF_ZONE_ID]):
        print("âš ï¸ CF API é…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•æ¸…é™¤ç¼“å­˜")
        return
    async with aiohttp.ClientSession() as session:
        headers = {
            "Authorization": f"Bearer {CF_API_TOKEN}",
            "Content-Type": "application/json"
        }
        payload = {"files": urls}
        async with session.post(f"{CF_API_URL}/zones/{CF_ZONE_ID}/purge_cache", json=payload, headers=headers) as resp:
            if resp.status == 200:
                print(f"âœ… è‡ªåŠ¨æ¸…é™¤ {len(urls)} ä¸ª URL ç¼“å­˜æˆåŠŸ")
            else:
                text = await resp.text()
                print(f"âš ï¸ è‡ªåŠ¨æ¸…é™¤ç¼“å­˜å¤±è´¥: {text}")

# -------------------------
# å¼‚æ­¥æ£€æµ‹ç¼“å­˜
# -------------------------
async def check_cache(session, url, filename, error_urls):
    try:
        async with session.head(url, timeout=15) as r:
            status = r.headers.get("cf-cache-status", "").upper()
            age = r.headers.get("age", "0")
            content_type = r.headers.get("content-type", "")
            if "text/html" in content_type:
                error_urls.append(url)
                return "âš ï¸ ERROR", "è¿”å› HTMLï¼Œå¯èƒ½æ— æ•ˆ"

            if status == "HIT":
                return "âœ… SUCCESS", age
            elif status == "MISS":
                if DOWNLOAD_IF_MISS:
                    if SAVE_FILE:
                        os.makedirs(DOWNLOAD_DIR, exist_ok=True)
                        file_path = os.path.join(DOWNLOAD_DIR, filename)
                    else:
                        tmp_file = tempfile.NamedTemporaryFile(delete=False)
                        file_path = tmp_file.name

                    async with session.get(url, timeout=60) as resp2:
                        with open(file_path, "wb") as f:
                            async for chunk in resp2.content.iter_chunked(1024*1024):
                                f.write(chunk)
                    if not SAVE_FILE:
                        os.remove(file_path)
                    await asyncio.sleep(HEAD_WAIT)

                    async with session.head(url, timeout=15) as r2:
                        status2 = r2.headers.get("cf-cache-status", "").upper()
                        age2 = r2.headers.get("age", "0")
                        return "âœ… SUCCESS", age2
                else:
                    return "ğŸŸ¡ MISS", age
            else:
                error_urls.append(url)
                return "âš ï¸ ERROR", f"æœªçŸ¥çŠ¶æ€ {status}"
    except Exception as e:
        error_urls.append(url)
        return "âš ï¸ ERROR", str(e)

# -------------------------
# å¼‚æ­¥ worker
# -------------------------
async def worker(sem, session, url, col, i, error_urls):
    async with sem:
        filename = os.path.basename(url)
        status, age = await check_cache(session, url, filename, error_urls)
        df.at[i, f"{col}_cache_status"] = status
        df.at[i, f"{col}_age"] = age

# -------------------------
# ä¸»å‡½æ•°
# -------------------------
async def main():
    sem = asyncio.Semaphore(MAX_CONCURRENT)
    async with aiohttp.ClientSession() as session:
        error_urls = []
        tasks = []
        for col in COLUMNS:
            for i, row in df.iterrows():
                url = row[col]
                if pd.notna(url):
                    tasks.append(worker(sem, session, url, col, i, error_urls))

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºç»Ÿä¸€è¿›åº¦æ¡
        for f in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc="æ£€æµ‹è¿›åº¦"):
            await f

        # æ‰¹é‡æ¸…é™¤ CF ç¼“å­˜
        if AUTO_PURGE_CF_CACHE and error_urls:
            print(f"âš ï¸ æ£€æµ‹åˆ° {len(error_urls)} ä¸ªé”™è¯¯ URLï¼Œå¼€å§‹æ‰¹é‡æ¸…é™¤ CF ç¼“å­˜...")
            await purge_cf_cache(error_urls)

    # ä¿å­˜æœ€ç»ˆ CSV
    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"âœ… æ£€æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {OUTPUT_CSV}")

# -------------------------
# æ‰§è¡Œ
# -------------------------
asyncio.run(main())
